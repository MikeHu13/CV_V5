{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMvqERPaAsM+z7ttRY1w0d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeHu13/CV_V5/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0haHqkjQws2"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model\n",
        "# setting up google drive\n",
        "import platform\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# set the correct root dir (precreate on google drive)\n",
        "root_dir = 'gdrive/My Drive/set_small/'\n",
        "\n",
        "\n",
        "\n",
        "# Import the chosen network\n",
        "conv_base = keras.applications.InceptionResNetV2( # Xception ...\n",
        "is actual top one\n",
        "weights='imagenet', # Load weights pre-trained on ImageNet.\n",
        "input_shape=(150, 150, 3),\n",
        "include_top=False) # Do not include the ImageNet classifier at ...\n",
        "the top.\n",
        "# Print strucutre in console\n",
        "conv_base.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDcazdxdROWv"
      },
      "source": [
        "# second part\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "# Output to console\n",
        "print('This is the number of trainable weights '\n",
        "'before freezing the conv base:',)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Freeze the conv base: set switch to freeze\n",
        "conv_base.trainable = False\n",
        "# Output to compare\n",
        "print('This is the number of trainable weights '\n",
        "'after freezing the conv base:')\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pvHQFPtRkNm"
      },
      "source": [
        "# load modules needed\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# set directories for training, validation and test set, here ...\n",
        "google drive\n",
        "base_dir = 'gdrive/My Drive/set_small/'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "# Set up constructor for data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "rescale=1./255,\n",
        "rotation_range=40,\n",
        "width_shift_range=0.2,\n",
        "height_shift_range=0.2,\n",
        "shear_range=0.2,\n",
        "zoom_range=0.2,\n",
        "horizontal_flip=True,\n",
        "fill_mode='nearest')\n",
        "# Test set only normalized, not augmented\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "# Collect the images and do the augmentation, labels are set to binary\n",
        "# augment training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "train_dir,\n",
        "target_size=(150, 150),\n",
        "batch_size=20,\n",
        "class_mode='binary')\n",
        "# augment validation data\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "validation_dir,\n",
        "target_size=(150, 150),\n",
        "batch_size=20,\n",
        "class_mode='binary')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DctrpUFPRmWc"
      },
      "source": [
        "# Elseif cases with fall back\n",
        "if TRANSFER:\n",
        "print('Using Transfer learning...')\n",
        "# parameters for model training\n",
        "model.compile(loss='binary_crossentropy',\n",
        "optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "metrics=['acc'])\n",
        "# do the training\n",
        "history = model.fit(\n",
        "train_generator,\n",
        "steps_per_epoch=100, # should be num_train/batch_size\n",
        "epochs=30,\n",
        "validation_data=validation_generator,\n",
        "validation_steps=50) # should be num_val/batch_size\n",
        "elif FINETUNE:\n",
        "print('Finetuning the model...')\n",
        "# set the parameters, here only freeze blocks 1 to 4\n",
        "# The loop freezes all layers UP TO a specific one\n",
        "conv_base.trainable = True\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "# Replace according to your architecture...\n",
        "#if layer.name == 'block5_conv1':\n",
        "if layer.name == 'conv5_block3_3_conv':\n",
        "set_trainable = True\n",
        "if set_trainable:\n",
        "layer.trainable = True\n",
        "else:\n",
        "layer.trainable = False\n",
        "# compile the model with given parameters\n",
        "model.compile(loss='binary_crossentropy',\n",
        "optimizer=optimizers.RMSprop(lr=1e-5),\n",
        "metrics=['acc'])\n",
        "# Fine tune the model and start training\n",
        "history = model.fit(\n",
        "train_generator,\n",
        "steps_per_epoch=100,\n",
        "epochs=100,\n",
        "validation_data=validation_generator,\n",
        "validation_steps=50)\n",
        "# Fallback if something wents wrong\n",
        "else:\n",
        "print('No freeze model, no other model')\n",
        "# save the trained model\n",
        "save_dir = os.path.join(root_dir, 'cats_dogs_net.h5')\n",
        "model.save(save_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vv0m5VIRoYN"
      },
      "source": [
        "# get the recorded data for training and validation looking at ...\n",
        "accuracy and loss\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "# plot the curves\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# test the accuraccy\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "test_dir,\n",
        "target_size=(150, 150),\n",
        "batch_size=20,\n",
        "class_mode='binary')\n",
        "# evaluate the test images with the model\n",
        "test_loss, test_acc = model.evaluate(test_generator, steps=50)\n",
        "# Output to console\n",
        "print('test acc:', test_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}